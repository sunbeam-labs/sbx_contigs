# -*- mode: Snakemake -*-

import csv
import numpy

rule contigs_by_taxa:
    input:
        summary_files = expand(str(ANNOTATION_FP/'summary'/'{sample}.tsv'), sample=Samples.keys()),
        taxa_nodes = Cfg['sbx_contigs']['taxa_nodes_fp'],
        taxa_names = Cfg['sbx_contigs']['taxa_names_fp'],
        taxa_sql = Cfg['sbx_contigs']['taxa_sql_fp']
    output:
        report = str(ANNOTATION_FP/Cfg['sbx_contigs']['taxa_of_interest']/'reports.txt')
    params:
        outdir = str(ANNOTATION_FP/Cfg['sbx_contigs']['taxa_of_interest']),
        db = Cfg['sbx_contigs']['blast_db'],
        min_contig_len = Cfg['sbx_contigs']['min_contig_len'],
        taxaName = Cfg['sbx_contigs']['taxa_of_interest'],
        contigs = expand(str(ANNOTATION_FP/Cfg['sbx_contigs']['taxa_of_interest']/'{sample}.txt'), sample=Samples.keys())
    script:
        sunbeam_dir + '/extensions/sbx_contigs/contigs_by_taxa.R'


## Make sure you update your samples.csv and sunbeam_config.yml before you preoceed

rule _contigs_selected:
    input:
        expand(str(ASSEMBLY_FP/Cfg['sbx_contigs']['taxa_of_interest']/'{sample}-contigs.fa'), sample=Samples.keys())

rule contigs_selected:
    input:
        contig = str(ASSEMBLY_FP/'contigs'/'{sample}-contigs.fa'),
        names = str(ANNOTATION_FP/Cfg['sbx_contigs']['taxa_of_interest']/'{sample}.txt')
    output:
        str(ASSEMBLY_FP/Cfg['sbx_contigs']['taxa_of_interest']/'{sample}-contigs.fa')
    shell:
        """
        seqtk subseq {input.contig} {input.names} > {output}
        """

## I think it's common practice to calcuate contigs coverage, so I re-organize the following rules

rule contigs_filter_by_length:
   input: 
        str(ASSEMBLY_FP/'contigs'/'{sample}-contigs.fa')
   output:
        str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}-contigs.fa')
   params:
        len = Cfg['sbx_contigs']['min_contig_len']
   shell:
        """
        vsearch --sortbylength {input} \
            --minseqlength {params.len} \
            --maxseqlength -1 \
            --notrunclabels \
            --output {output}
        """

rule contigs_mapping:
    input:
        #contig = str(ASSEMBLY_FP/Cfg['sbx_contigs']['taxa_of_interest']/'{sample}-contigs.fa'),
        contig = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}-contigs.fa'),
        reads = expand(str(QC_FP/'decontam'/'{{sample}}_{rp}.fastq.gz'),rp = Pairs)
    output:
        sam = temp(str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}.sam')),
        bam = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}.sorted.bam'),
        bai = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}.sorted.bam.bai'),
        bcf = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'{sample}.raw.bcf'),
        counts = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.counts'),
        cov = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.coverage'),
        depth = str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.depth')
    threads: 8
    shell:
        """
        bwa index {input.contig}

        bwa mem -M -t {threads} {input.contig} {input.reads} -o {output.sam}
        samtools view -@ {threads} -b -F4 {output.sam} | samtools sort -@ {threads} > {output.bam}
        samtools index {output.bam} {output.bai}
        samtools mpileup -gf {input.contig} {output.bam} | bcftools call -Ob -v -c - > {output.bcf}
        
        samtools depth -aa {output.bam} > {output.depth}
        samtools idxstats {output.bam} | cut -f 1,3 | sed '$ d' > {output.counts}
        bedtools genomecov -ibam {output.bam} -g {input.contig} -bg > {output.cov}
        """

rule _contigs_mapping:
    input:
        expand(str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.{suffix}'), 
               sample=Samples.keys(), suffix = ['coverage', 'depth', 'counts'])

rule _all_coverage:
    input:
        expand(str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.csv'), 
               sample=Samples.keys())

def get_coverage(input_fp, sample, output_fp):
    with open(input_fp) as f:
        reader = csv.reader(f, delimiter='\t')    
    	data = {}
    	for row in reader:
            if not data.get(row[0]):
                data[row[0]] = []
            data[row[0]].append(int(row[2]))
    
    # summarize stats for all segments present and append to output
    output_rows = []
    for segment in data.keys():
        minval     = numpy.min(data[segment])
        maxval     = numpy.max(data[segment])
        mean       = numpy.mean(data[segment])
        median     = numpy.median(data[segment])
        stddev     = numpy.std(data[segment])
        gen_cov    = len(list(filter(lambda x: x!=0, data[segment])))
        gen_length = len(data[segment])
        row = [sample, segment, minval, maxval, mean, median, stddev, gen_cov, gen_length]
        output_rows.append(row)

    # write out stats per segment per sample
    fields = ['sample','contig', 'min', 'max', 'mean', 'median', 'stddev', 'coverage', 'length']
    with open(output_fp, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(fields)
        writer.writerows(output_rows)


rule get_coverage:
    input:
        str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.depth')
    output:
        str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.csv')
    run:
        get_coverage(input[0], wildcards.sample, output[0])

rule summarize_coverage:
    input:
        expand(str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'{sample}.csv'), 
               sample = Samples.keys())
    output:
        str(ASSEMBLY_FP/'sbx_contigs'/Cfg['sbx_contigs']['min_contig_len']/'reports'/'coverage.csv')
    shell:
        "(head -n 1 {input[0]}; tail -q -n +2 {input}) > {output}"
